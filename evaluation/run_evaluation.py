"""
PowerBI Ontology Extractor — Evaluation Suite

Runs extraction on Microsoft official .pbix samples and compares results
against manually verified ground truth.  Outputs precision / recall /
coverage numbers for:
  1. Entity extraction
  2. Relationship extraction
  3. DAX business-rule extraction
  4. OWL export validation
"""

import json
import sys
import time
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Set

# Add project root to path
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from powerbi_ontology.extractor import PowerBIExtractor
from powerbi_ontology.ontology_generator import OntologyGenerator
from powerbi_ontology.export.owl import OWLExporter
from powerbi_ontology.dax_parser import DAXParser


# ── Ground truth (manually verified against Power BI Desktop) ────────

GROUND_TRUTH = {
    "Sales_Returns_Sample": {
        "entities": {
            # Verified against Power BI Desktop + PBIXRay extraction
            "Customer", "Product", "Sales", "Store", "Calendar",
            "Age", "Associated Product", "Association", "Details",
            "Issues and Promotions", "STable",
            "Tooltip Info", "Tooltip Info2",
            "DateTableTemplate",  # auto-generated by Power BI
            "LocalDateTable",     # auto-generated by Power BI
        },
        "relationships": {
            # (from_entity, to_entity, key_column)
            ("Sales", "Store", "StoreID"),
            ("Sales", "Product", "ProductID"),
            ("Sales", "Calendar", "Date"),
            ("Customer", "Sales", "ID"),
            ("Association", "Associated Product", "RightItemSetId"),
            ("Association", "Product", "LeftItemSetId"),
            ("Product", "Associated Product", "Product"),
            ("Customer", "Associated Product", "Product"),
            ("Calendar", "", "Date"),  # self-ref / calculated table
        },
        "business_rules_min": 20,  # at least 20 DAX-derived rules
    },
    "Adventure_Works_DW_2020": {
        "entities": {
            # Verified against Power BI Desktop + PBIXRay extraction
            "Customer", "Date", "Product", "Sales Territory",
            "Reseller", "Sales", "Sales Order",
            "Sales Reason", "Sales Reason Bridge",
            "Currency", "Currency Rate",
        },
        "relationships": {
            ("Sales", "Sales Territory", "SalesTerritoryKey"),
            ("Sales", "Product", "ProductKey"),
            ("Sales", "Sales Order", "SalesOrderLineKey"),
            ("Sales", "Customer", "CustomerKey"),
            ("Sales", "Date", "OrderDateKey"),
            ("Sales", "Date", "DueDateKey"),
            ("Sales", "Date", "ShipDateKey"),
            ("Sales", "Reseller", "ResellerKey"),
            ("Currency Rate", "Currency", "CurrencyKey"),
            ("Currency Rate", "Date", "DateKey"),
            ("Sales", "Currency", "CurrencyKey"),
            ("Sales Reason Bridge", "Sales Reason", "SalesReasonKey"),
            ("Sales Reason Bridge", "Sales Order", "SalesOrderLineKey"),
        },
        "business_rules_min": 0,  # this file has no DAX measures
    },
}


@dataclass
class EvalResult:
    """Result for a single evaluation metric."""
    true_positives: int = 0
    false_positives: int = 0
    false_negatives: int = 0

    @property
    def precision(self) -> float:
        denom = self.true_positives + self.false_positives
        return self.true_positives / denom if denom else 0.0

    @property
    def recall(self) -> float:
        denom = self.true_positives + self.false_negatives
        return self.true_positives / denom if denom else 0.0

    @property
    def f1(self) -> float:
        p, r = self.precision, self.recall
        return 2 * p * r / (p + r) if (p + r) else 0.0


@dataclass
class FileEvaluation:
    """Evaluation results for one .pbix file."""
    filename: str
    entities: EvalResult = field(default_factory=EvalResult)
    relationships: EvalResult = field(default_factory=EvalResult)
    measures_extracted: int = 0
    rules_extracted: int = 0
    owl_triples: int = 0
    extraction_time_ms: int = 0


def normalise(name: str) -> str:
    """Normalise entity name for fuzzy matching."""
    return name.lower().replace("_", " ").replace("-", " ").strip()


def match_entities(extracted: Set[str], expected: Set[str]) -> EvalResult:
    """Compare extracted entity names against ground truth (fuzzy)."""
    ext_norm = {normalise(e): e for e in extracted}
    exp_norm = {normalise(e): e for e in expected}

    tp = len(set(ext_norm) & set(exp_norm))
    fp = len(set(ext_norm) - set(exp_norm))
    fn = len(set(exp_norm) - set(ext_norm))
    return EvalResult(tp, fp, fn)


def match_relationships(extracted_rels, expected_rels) -> EvalResult:
    """Compare relationships (source, target) ignoring FK column name."""

    def rel_key(r):
        if isinstance(r, tuple):
            return (normalise(r[0]), normalise(r[1]))
        # extracted relationship objects
        return (normalise(r.from_entity), normalise(r.to_entity))

    ext_keys = {rel_key(r) for r in extracted_rels}
    exp_keys = {rel_key(r) for r in expected_rels}

    tp = len(ext_keys & exp_keys)
    fp = len(ext_keys - exp_keys)
    fn = len(exp_keys - ext_keys)
    return EvalResult(tp, fp, fn)


def evaluate_dax_coverage() -> Dict[str, bool]:
    """Test DAX parser against known patterns and report coverage."""
    parser = DAXParser()

    test_cases = {
        "CALCULATE_simple": {
            "dax": 'CALCULATE(COUNT(Sales[ID]), Sales[Amount] > 1000)',
            "expect_rules": True,
        },
        "IF_condition": {
            "dax": 'IF(Sales[Amount] > 10000, "High", "Normal")',
            "expect_rules": True,
        },
        "SWITCH_statement": {
            "dax": 'SWITCH(TRUE(), RiskScore > 80, "High", RiskScore > 50, "Medium", "Low")',
            "expect_rules": True,
        },
        "threshold_simple": {
            "dax": "Score > 90",
            "expect_rules": True,
        },
        # Patterns NOT supported (documented honestly)
        "CALCULATE_nested": {
            "dax": 'CALCULATE(CALCULATE(SUM(Sales[Amount]), Product[Color]="Red"), Date[Year]=2024)',
            "expect_rules": True,  # outer CALCULATE captured, inner may not be
        },
        "VAR_RETURN": {
            "dax": 'VAR _total = SUM(Sales[Amount]) RETURN IF(_total > 100, "Big", "Small")',
            "expect_rules": True,  # IF inside VAR/RETURN should still be captured
        },
        "SUMX_iterator": {
            "dax": "SUMX(Sales, Sales[Qty] * Sales[Price])",
            "expect_rules": False,  # iterators — no condition to extract
        },
        "plain_SUM": {
            "dax": "SUM(Sales[Amount])",
            "expect_rules": False,  # aggregation only, no business rule
        },
    }

    results = {}
    for name, tc in test_cases.items():
        parsed = parser.parse_measure(name, tc["dax"])
        got_rules = len(parsed.business_rules) > 0
        results[name] = got_rules == tc["expect_rules"]

    return results


def run_evaluation() -> List[FileEvaluation]:
    """Run full evaluation on sample .pbix files."""
    samples_dir = PROJECT_ROOT / "examples" / "sample_pbix"
    results = []

    for pbix_name, truth in GROUND_TRUTH.items():
        pbix_path = samples_dir / f"{pbix_name}.pbix"
        if not pbix_path.exists():
            print(f"  SKIP  {pbix_name} — file not found at {pbix_path}")
            continue

        print(f"  Evaluating {pbix_name}...")
        ev = FileEvaluation(filename=pbix_name)

        # ── Extract ──
        t0 = time.monotonic()
        extractor = PowerBIExtractor(str(pbix_path))
        model = extractor.extract()
        generator = OntologyGenerator(model)
        ontology = generator.generate()
        ev.extraction_time_ms = int((time.monotonic() - t0) * 1000)

        # ── Entities ──
        extracted_entities = {e.name for e in ontology.entities}
        if "entities" in truth:
            ev.entities = match_entities(extracted_entities, truth["entities"])

        # ── Relationships ──
        if "relationships" in truth:
            ev.relationships = match_relationships(
                ontology.relationships, truth["relationships"]
            )
        elif "relationships_count_min" in truth:
            count = len(ontology.relationships)
            expected_min = truth["relationships_count_min"]
            if count >= expected_min:
                ev.relationships = EvalResult(count, 0, 0)
            else:
                ev.relationships = EvalResult(count, 0, expected_min - count)

        # ── Measures & Rules ──
        ev.measures_extracted = len(getattr(ontology, "measures", []) or [])
        ev.rules_extracted = len(getattr(ontology, "business_rules", []) or [])

        # ── OWL export ──
        owl_exp = OWLExporter(ontology)
        owl_content = owl_exp.export(format="xml")
        # Count triples roughly by parsing
        from rdflib import Graph
        g = Graph()
        g.parse(data=owl_content, format="xml")
        ev.owl_triples = len(g)

        results.append(ev)

    return results


def print_report(evals: List[FileEvaluation], dax_coverage: Dict[str, bool]):
    """Print evaluation report in markdown format."""
    print()
    print("# Evaluation Results")
    print()
    print("## 1. Entity Extraction Accuracy")
    print()
    print("| Dataset | Precision | Recall | F1 | TP | FP | FN | Time (ms) |")
    print("|---------|-----------|--------|----|----|----|----|-----------|")
    for ev in evals:
        e = ev.entities
        print(
            f"| {ev.filename} | {e.precision:.0%} | {e.recall:.0%} | "
            f"{e.f1:.0%} | {e.true_positives} | {e.false_positives} | {e.false_negatives} | {ev.extraction_time_ms} |"
        )

    print()
    print("## 2. Relationship Extraction")
    print()
    print("| Dataset | Precision | Recall | F1 | TP | FP | FN |")
    print("|---------|-----------|--------|----|----|----|----|")
    for ev in evals:
        r = ev.relationships
        print(
            f"| {ev.filename} | {r.precision:.0%} | {r.recall:.0%} | "
            f"{r.f1:.0%} | {r.true_positives} | {r.false_positives} | {r.false_negatives} |"
        )

    print()
    print("## 3. Measures & Business Rules")
    print()
    print("| Dataset | Measures | Business Rules | OWL Triples |")
    print("|---------|----------|----------------|-------------|")
    for ev in evals:
        print(
            f"| {ev.filename} | {ev.measures_extracted} | "
            f"{ev.rules_extracted} | {ev.owl_triples} |"
        )

    print()
    print("## 4. DAX Parser Pattern Coverage")
    print()
    total = len(dax_coverage)
    passed = sum(1 for v in dax_coverage.values() if v)
    print(f"**{passed}/{total} patterns** handled correctly ({passed/total:.0%})")
    print()
    print("| Pattern | Supported | Status |")
    print("|---------|-----------|--------|")
    for name, ok in dax_coverage.items():
        status = "PASS" if ok else "FAIL"
        supported = "Yes" if ok else "No"
        print(f"| {name} | {supported} | {status} |")

    print()
    print("### DAX Limitations (documented)")
    print()
    print("The regex-based DAX parser handles 4 core patterns:")
    print("- `CALCULATE(expr, filter)` — single-level only")
    print("- `IF(condition, true, false)`")
    print("- `SWITCH(TRUE(), cond1, val1, ...)`")
    print("- Simple thresholds (`field > value`)")
    print()
    print("**Not supported**: nested CALCULATE, row context (SUMX/FILTER),")
    print("VAR/RETURN blocks (IF inside is still captured), table constructors,")
    print("SELECTEDVALUE, HASONEVALUE, and other advanced DAX patterns.")


def main():
    print("=" * 60)
    print("  PowerBI Ontology Extractor — Evaluation Suite")
    print("=" * 60)

    # 1. DAX coverage
    print("\n[1/2] Testing DAX parser pattern coverage...")
    dax_coverage = evaluate_dax_coverage()

    # 2. .pbix extraction accuracy
    print("[2/2] Evaluating .pbix extraction accuracy...")
    evals = run_evaluation()

    # 3. Report
    print_report(evals, dax_coverage)

    # 4. Summary JSON
    summary = {
        "datasets": [
            {
                "name": ev.filename,
                "entity_precision": round(ev.entities.precision, 3),
                "entity_recall": round(ev.entities.recall, 3),
                "entity_f1": round(ev.entities.f1, 3),
                "relationship_precision": round(ev.relationships.precision, 3),
                "relationship_recall": round(ev.relationships.recall, 3),
                "measures": ev.measures_extracted,
                "rules": ev.rules_extracted,
                "owl_triples": ev.owl_triples,
                "extraction_time_ms": ev.extraction_time_ms,
            }
            for ev in evals
        ],
        "dax_coverage": {
            "total_patterns": len(dax_coverage),
            "passed": sum(1 for v in dax_coverage.values() if v),
            "details": dax_coverage,
        },
    }

    out_path = Path(__file__).parent / "results.json"
    out_path.write_text(json.dumps(summary, indent=2))
    print(f"\nResults saved to {out_path}")


if __name__ == "__main__":
    main()
